# organization 6

# 5.7 Supervised Learning Algorithms

## simply says Supervised Learning Algoritms
- kNN: data point에서 가장 가까운(nearest) Training data point, 최근접 이웃을 찾는 알고리즘. 작은 data set일 경우에 기본모델로서 좋고 이해하기 쉽다.
- linear model: 선형적인 직선이나 평면, 초평면 등을 이용하여 출력을 찾는 알고리즘. 첫 번째로 시도하기 좋으며, 대용량 data set과 high dimension data set에 가능하다.
- naive bayes: 데이터의 특성을 독립적이라 가정하여 각 상황의 확률을 계산하여 결과를 출력한다. 분류 문제에만 적용할 수 있다. 대용량 데이터 셋과 고차원 데이터에 사용가능하다. 선형모델보다 훨씬 빠르나 정확도는 떨어진다.
- decision tree: 데이터를 이진 분류하는 것을 반복하여 최종적으로 결과를 출력한다. 매우 빠르고 데이터 스케일의 조정이 필요 없다. 시각화하기 좋고 설명하기 쉬우나 과대적합되는 경향이 있다.
- random forest: 과대적합된 단일 결정 트리를 여러 개 묶어 하나의 모델로 만든 것. 결정 트리 하나보다 거의 항상 좋은 성능을 낸다. 매우 안정적이고 강력하며 데이터 스케일의 조정이 필요가 없다. 다만 시간이 오래 걸리고 텍스트같은 고차원 희소 데이터에는 잘 맞지 않는다.
- gredient boosting decision tree: 깊이가 깊지 않은 단일 결정 트리를 여러 개 묶어 하나의 모델로 만든 것. 랜덤 포레스트보다 조금 더 성능이 좋다. 랜덤 포레스트보다 학습은 느리나 예측은 빠르고 메모리를 적게 사용한다. 다만 랜덤 포레스트보다 매개변수의 튜닝이 많이 필요하다.
- support vector machine: 클래스 사이의 경계에 위치한 데이터 포인트(서포트 벡터) 간의 거리를 최대화하여 결과를 찾는 알고리즘. 비슷한 의미의 특성으로 이루어진 중간 규모 데이터 세트에는 잘 맞지만, 데이터 스케일의 조정이 필요하고 매개변수에 민감하다.
- neural network: 대용량 데이터 세트에서 매우 복잡한 모델을 만들 수 있다. 다만 매개변수의 선택과 데이터 스케일에 민감하며, 큰 모델에는 학습이 오래 걸린다.


## kNN
가장 가까운 K개를 찾아 K개중 가장 많은 분류 집단으로 대상을 분류하는 알고리즘.

거리 개념이기 때문에 수치 데이터를 활용하며, 스케일이 요인에 영향을 줄 수 있기 때문에 동질 데이터를 활용해야한다.

Training 단계가 따로 필요 없기 때문에 정답이 있는것과 없는것을 같이 넣는다. 현재 데이터 트랜드가 계속 반영되다 보니까 따로 증강 학습이 필요없다. 따라서 Training 단계는 매우 빠르지만, 분류 단계에서는 모델이 따로 없기 때문에 매우 느리다. 또한 샘플 데이터가 항상 메모리에 올려져 있어야 하기 때문에 많은 메모리가 필요하다.

### 적당한 k 선택
K를 선택할때, over 혹은 under fitting 문제가 발생할 수 있으므로, 잘 선택해야한다. 일반적으로는 제곱근을 사용한다.

### 동질 데이터 준비
각 요인간의 스케일 차이가 결과에 영향을 줄 수 있기때문에 동질 데이터로 맞춰줘야한다. min-max normalization은 아래 수식을 보면 알수 있듯이, 값이 오밀조밀 몰려있는 요인이 거리에 영향을 많이 줄수 있다. 이 단점을 보안하기 위한 방법이 z-score standardization 이다. 분산값으로 나눠주는 방법을 사용한다. 마지막으로 dummy coding 방법은 명목형 데이터를 수치화 하는건데 차원의 저주를 조심해야한다.

## Decision tree
의사결정트리는 분류와 같은 의사결정을 수행할 때, 나무와 같이 가지치기를 함으로써 분류하는 방법이다. 과거 수집된 데이터들을 분석하여 이들 사이에 존재하는 패턴을 속성의 조합으로 나타내는 분류 모형이다.

- 의사결정트리 분류 절차
```
새로운 데이터 분류 (classification)
해당 범주 값 예측
트리 구조의 일반화된 지식 추출
```

범주형과 연속형 두 가지 데이터 유형으로 나뉜다. 범주형은 분류나무로써 의사결정트리를 구성하고, 연속형은 회귀나무로 의사결정트리를 구성한다. 하지만 의사결정트리는 회귀모델의 정확도가 낮기 때문에 주로 분류의 목적으로 사용한다.


## Decision tree
본질적으로 플로우 차트이기 때문에 근거가 투명해야 하는 분류 과정이 필요한 경우 많이 쓰인다 . Recursive Partitioning 이라는 heuristic을 사용해 만들어 진다. 결측치, 명목 속성, 수치를 처리할수 있는 자동성이 높은 학습 이다. 모든 문제에 적합한 분류기라 볼수 있다. 반면에 모델이 쉽게 과적합화나 과소적합화될 가능성이 있다. 또한 기울기가 없이 축 평형으로 구분하기 때문에 일부 관계를 모델화하는데 문제가 있다.

### entropy
Entropy를 사용해서 Decision tree의 종료조건을 찾는다. 0은 완전히 하나임을 의미하고 1은 다종성이 존재함을 나타낸다. Entropy를 계산한 후 S1으로 분할 했을 때와 S2로 분할 했을떄의 Entropy 차이를 계산한다.(Information gain :속성 A에 따라 예제를 분할 함으로써 기대되는 Entropy 감소량/ 클수록 좋음)

### pruning
각예제를 작은 분할로 나누기 때문에 결정 트리는 계속적으로 자랄 수가 있다. 그러나 트리가 너무 커진다면 많은 결정이 생기며, 훈련 데이터에 과적합화가 된다. 이에 Pre-pruning이라고 어느 정도만 하고 조기에서 가지치기 과정을 멈추는 방법이 있다. 반대로 Post-pruning은 가지를 한번 더 치고 Gain이 낮으면 그만 하는 방법이다. 이 방법은 Overfitting될 우려가 있다.

### C5.0알고리즘은 먼저 트리를 성장 시킨 후에 pruning을 실행한다.
- 과적합해 크게 성장시킨다.
- 분류 오차를 가진 노드와 가지를 제거하나다.
- subtree raising 과 subtree replacement 한다.
- 모델의 성능 향상(Adaptive boosting) : 샘플데이터 별로 트리 모양이 달라지니까, 많은 결정 트리를 만들고 각 예제에 대해 최적의 범주에 투표.
- 고비용의 실수 만들기(Error_cost) : 같은 error라도 cost를 다르게 준다.


## Linear regression
회귀는 수치에 대한 해결을 도와준다. 입력 X에 대한 출력 값 Y를 예측하기 위해 선형 관계를 사용한다. 이제 어떻게 우리의 입력에 맞는 선을 결정하고, 그 선이 모르는 데이터들에 대해서도 잘 예측할 것인지 알기 위해 오류를 측정할 필요가 있다. 대표적인 오류 측정 방법이 최소 자승법이다. 최소 승법은 잔차의 제곱 합이 최소일때 모델이 데이터에 가장 적합하다고 판단한다.

선형 회귀에 적합 시키기 위해서는 먼저 데이터를 이해하고 두변수 사이의 상관관계가 있는지 파악해야한다. 이후 상관관계가 있을 경우 선형 모델 유틸리티를 사용해 선형 회귀 모델을 구한다. 다시말하면 선형회귀는 산포도가 선형 패턴을 가지고, 상관 관계가 보통 보다 강한 데이터 인 경우를 예측하기 위해 사용된다.



## SVM
서포트 벡터 머신(SVM, Support Vector Machine)이란 주어진 데이터가 어느 카테고리에 속할지 판단하는 이진 선형 분류 모델입니다.

데이터를 선형으로 분리하는 최적의 선형 결정 경계를 찾는 알고리즘이다.

분류나 회규 분석에 사용이 가능하며, 특히 분류쪽의 성능이 뛰어나기 때문에 주로 분류에 많이 사용이 된다.

training time이 길다. 사이즈가 큰 데이터 셋에는 적합하지 않다. 또한 노이즈가 많은 데이터 셋에서는 오버피팅이 될 수 있어 적합하지 않다. (이럴경우 나이브 베이즈가 더 적합할 수 있다.)

### maximize margin
정답은 가운데 선이 Margin을 최대화하기 때문입니다. Margin이란 선과 가장 가까운 양 옆 데이터와의 거리입니다. 선과 가장 가까운 포인트를 서포트 벡터(Support vector)라고 합니다. 즉, Margin은 구분하는 선과 서포트 벡터와의 거리를 의미합니다. 또한 이렇게 두 데이터를 구분하는 선을 Decision Boundary라고 합니다. 본 포스트에서는 Decision Boundary를 편의상 구분선이라고 부르겠습니다. 이제 아래 그림을 보겠습니다.

### Roustness
세 개의 선 중 어떤 것이 Margin을 최대화할까요? 바로 가운데 선입니다. 왼쪽 선은 파란 서포트 벡터와의 거리는 길지만 빨간 서포트 벡터와의 거리는 짧습니다. 반면 가운데 선은 빨간 서포트 벡터, 파란 서포트 벡터와의 거리가 같습니다. 각 서포트 벡터와의 Margin을 최대화하는 방향으로 구분선을 잡아야 하므로 가운데 선이 가장 적절한 구분 선이 됩니다. 이렇게 양 옆 서포트 벡터와의 Margin을 최대화하면 robustness도 최대화가 됩니다. 데이터 과학에서 로버스트(robust)하다는 말을 많이 씁니다. 로버스트하다는 것은 아웃라이어(outlier)의 영향을 받지 않는다는 뜻입니다. 아래 예시를 들어 설명해보겠습니다.

### data exact classify
둘 중 어떤 선이 SVM의 구분선으로 적절할까요? 표시가 된 아래 구분선입니다. 첫 번째 구분선이 Margin이 더 크긴합니다. 하지만 첫 번째 구분선은 데이터를 정확히 구분하지 못합니다. 구분선 아래 빨간 포인트가 있으니까요. SVM은 무작정 Margin을 크게 하는 구분선을 택하는 것이 아닙니다. 우선, 데이터를 정확히 분류하는 범위를 먼저 찾고, 그 범위 안에서 Margin을 최대화하는 구분선을 택하는 것입니다.

### outlier processing
표시된 맨 오른쪽 구분선이 가장 적합합니다. 이전 예제처럼 SVM은 우선 두 데이터를 정확히 구분하는 선을 찾습니다. 하지만 두 데이터를 정확히 구분하는 직선은 없습니다. 이럴 땐 SVM이 어느 정도 outlier를 무시하고 최적의 구분선을 찾습니다. 위 예제에서 빨간 포인트 사이에 섞인 파란 포인트는 outlier로 취급해서 무시하고 Margin을 최대화하는 구분선을 찾은 것입니다.

### kernel trick
x와 y축으로 이루어진 왼쪽 좌표를 먼저 보겠습니다. 빨간 포인트와 파란 포인트를 구분할 수 있는 linear line은 없습니다. 아웃라이어를 무시하고 그릴 수 있는 상황도 아닙니다. 이럴 땐 어떻게 해야 할까요? 차원을 바꿔주어 구분선을 그릴 수 있습니다. z = x^2 + y^2라고 하고, 오른쪽처럼 z와 x로 이루어진 좌표를 그립니다. 그러면 왼쪽 그래프가 오른쪽 그래프처럼 바뀝니다. z는 원점으로부터 해당 데이터까지 떨어진 거리의 제곱과 같습니다. 따라서 빨간 포인트는 원점으로부터의 거리가 짧아 z가 작고, 파란 포인트는 원점으로부터의 거리가 길어 z가 큽니다. 왼쪽 그래프에서는 linear line을 그릴 수 없었는데, 오른쪽 그래프에서는 linear line으로 구분할 수 있습니다. 즉, 선형으로 분류할 수 있게 된 것입니다. (It can be linearly separable)


오른쪽 그래프에서 linear하게 그린 구분선은 왼쪽에서 원형으로 된 구분선과 동일합니다. 차원이 달라져서 구분선의 모양이 다른 것뿐입니다. x, y로만 이루어진 평면을 x, y, z 평면으로 차원 확대를 하니 구분선을 쉽게 그릴 수 있었습니다. 이렇듯 저차원 공간(low dimensional space)을 고차원 공간(high dimensional space)으로 매핑해주는 작업을 커널 트릭(Kernel Trick)이라고 합니다.

### C, Gamma
일반적으로 가장 많이 사용되는 SVM은 radial basis unction(RBF) kernel을 사용한 SVM이다. RBF kernel SVM의 hyper parameters에는 C, gamma가 있다. 보통 trial and error 즉, grid search라는 경험적인 방법에 의해서 선택한다.

C가 크면 training point를 정확히 구분해준다. C가 작으면 smooth한 decision boundary를 그려준다.

C가 작으면 Margin이 커진다.

즉, C가 크면 decision boundary는 더 굴곡지고, C가 작으면 deciosion boundary는 직선에 가깝다.

gamma값이 크면 reach가 좁다. 멀리 있는 포인트는 영향이 없어 가까이 있는 포인트 하나하나의 영향이 상대적으로 커진다. decision boundary는 굴곡진다.

gamma값이 작으면 reach가 멀다. 대부분의 포인트가 decision boundary에 영향을 준다.

### Gaussian RBF kernel
커널에는 Polynomial kernel, sigmoid kernel, gaussian RBF kernel 등 여러 가지가 있다.


## 


------------

# 추가내용

# 시그모이드 함수
```
sigmoid function
hard sigmoid function
```

- s자형 또는 sigmoid 형태이다. 
- 단조증가 혹은 단조감소이다. 
- 변곡점 1개이다.


# logistic regression
- 목적은 일반적인 회귀 분석의 목표와 동일하다.
- 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용한다.
- 종속 변수가 범주형 데이터를 대상으로 하며 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류기법으로도 볼 수 있다.(선형 회귀 분석과 다른 점)



------

# presentation

# 5.7
지도 학습 알고리즘은 입력 예제의 훈련 세트가 주어지면 일부 입력을 일부 출력과 연결하는 방법을 배우는 학습 알고리즘이다.

# 5.7.1 probablistic supervised learning
이 책에 나오는 대부분의 지도 학습 알고리즘은 확률 분포 추정을 기반으로 한다.
MLE를 사용해 최상의 매개 변수 벡터를 찾기만 하면 된다.

선형 회귀는 패밀리다.

다른 확률 분포 패밀리를 정의하여 분류 시나리오에 대한 선형 회귀를 일반화할 수 있다. 이는 베르누이 함수의 특징을 가진다.

선형 회귀에 사용한 실수값에 대한 정규 분포는 평균으로 모수화된다. 

이진 변수에 대한 분포는 항상 0과 1사이여야 하기 때문에 어렵다. 이 경우에는 로지스틱 시그모이드 함수를 사용하여 선형 함수의 출력을 구간(0, 1)로 스쿼시한다. 그리고 그 값을 확률로 해석한다.

선형 회귀의 경우 정규 방정식을 풀어 최적의 가중치를 찾을 수 있다. 

로지스틱 회귀는 조금 어렵다. 최적의 가중치를 위한 확실한 솔루션은 없다.

하지만 log-likelihood를 최대화하여 찾을 수 있다. 또한 Nagative log-likelihood를 최소화하여 찾을 수도 있다.


# 5.7.2 SVM



#5.7.3 other simple supervised learning algorithms

## kNN

## decision tree







----------

# Reference
- [지도학습 알고리즘: 정리](https://kolikim.tistory.com/25)
- [machine learning 실무. 지도 학습 알고리즘 1](https://medium.com/@osm0644/machine-learning-%EC%8B%A4%EB%AC%B4-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%981-3fba2b0afbd3)
- [머신러닝 - 2. 서포트 벡터 머신(SVM) 개념](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-2%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM)
- [시그모이드 함수](https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%ED%95%A8%EC%88%98)
- [logistic regression](https://ko.wikipedia.org/wiki/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80)
- [* 머신러닝. 9. 머신러닝 학습 방법(part 4) - SVM(1)](http://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220845107089&categoryNo=31&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)
- [SVM의 사용자로서 꼭 알아야 할 것들. 매개변수 C와 gamma](https://bskyvision.com/163)
- [* ML #7 머신러닝 kNN 알고리즘 장단점](https://muzukphysics.tistory.com/131?category=1111219)
- [* ML #8 머신러닝 SVM 기본 개념과 장단점](https://muzukphysics.tistory.com/135)