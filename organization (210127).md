# 5.2.2 Regularization

무료 점심식사 정리는 우리가 특정 작업에서 잘 수행할 수 있도록 기계 학습 알고리듬을 설계해야 한다는 것을 암시한다. 우리는 일련의 선호도를 학습 알고리듬으로 구축함으로써 그렇게 한다. 이러한 선호도가 알고리듬이 해결하도록 요구하는 학습 문제와 일치하면 더 나은 성능을 발휘한다.

지금까지 우리가 구체적으로 논의한 학습 알고리듬을 수정하는 유일한 방법은 학습 알고리듬이 선택할 수 있는 솔루션의 가설 공간에서 함수를 추가하거나 제거하여 모델의 표현 능력을 증가시키거나 감소시키는 것이다. 회귀 문제에 대한 다항식의 정도를 증가 또는 감소시키는 구체적인 예를 제시했다. 우리가 지금까지 설명한 관점은 지나치게 단순하다.

알고리듬의 동작은 우리가 가설 공간에서 허용되는 함수 집합을 얼마나 크게 만드는가에 의해서뿐만 아니라, 그러한 함수의 특정한 정체성에 의해서도 강하게 영향을 받는다. 지금까지 연구한 학습 알고리듬인 선형 회귀에는 입력의 선형 함수 집합으로 구성된 가설 공간이 있다. 이러한 선형 함수는 입력과 출력 사이의 관계가 실제로 선형에 가까운 문제에 매우 유용할 수 있다. 그것들은 매우 비선형적인 방식으로 작용하는 문제에는 덜 유용하다. 예를 들어, 선형 회귀 분석을 사용하여 x에서 sin(x)을 예측하려고 하면 선형 회귀 분석이 잘 수행되지 않을 것이다. 따라서 우리는 알고리즘이 솔루션을 끌어낼 수 있는 기능의 종류를 선택하고 이러한 함수의 양을 제어함으로써 알고리듬의 성능을 제어할 수 있다.

우리는 또한 학습 알고리듬이 가설 공간에서 다른 솔루션에 대한 선호도를 제공할 수 있다. 이는 두 기능이 모두 적용 가능하지만 한 기능이 선호된다는 것을 의미한다. 선호되지 않는 솔루션은 선호되는 솔루션보다 교육 데이터에 훨씬 더 적합한 경우에만 선택됩니다.

예를 들어, 우리는 체중 감소를 포함하도록 선형 회귀에 대한 훈련 기준을 수정할 수 있다. 체중 감소와 함께 선형 회귀를 수행하기 위해, 우리는 훈련의 평균 제곱 오차와 가중치가 더 작은 제곱 L2 규범을 갖는 선호도를 나타내는 기준 J(w)로 구성된 합계를 최소화한다. 구체적으로,

J(w) = MSE(Train) + (lambda)w^(T) w

여기서 α는 더 작은 무게에 대한 우리의 선호의 강도를 조절하는 미리 선택된 값입니다. α = 0일 때, 우리는 선호하지 않으며, α가 클수록 가중치가 작아진다. J(w)를 최소화하면 훈련 데이터를 적합시키는 것과 작은 것 사이의 절충을 이루는 가중치를 선택할 수 있다. 따라서 기울기가 더 작거나 더 적은 기능에 무게를 두는 솔루션을 얻을 수 있습니다. 가중치 감소를 통해 모델의 과적합 또는 과소적합 경향을 제어할 수 있는 방법의 예로서, 서로 다른 α 값을 갖는 고차원 다항식 회귀 모델을 훈련할 수 있다. 결과는 그림 5.5를 참조하십시오.

그림 5.5: 우리는 그림 5.2의 예제 훈련 세트에 고차원 다항식 회귀 모델을 적합시킨다. 실제 함수는 2차 함수이지만 여기서는 정도가 9인 모형만 사용합니다. 우리는 이러한 고차원적 모델이 과적합되지 않도록 체중 감소량을 변화시킨다. (왼쪽)α가 매우 크면 모델이 경사가 전혀 없는 함수를 학습하도록 할 수 있다. 이것은 상수 함수만 나타낼 수 있기 때문에 적합하지 않습니다. (중앙)α의 중간값으로 학습 알고리즘은 일반 형상이 맞는 곡선을 복구한다.
모형이 훨씬 더 복잡한 모양을 가진 함수를 나타낼 수 있지만, 체중 감소는 더 작은 계수로 설명되는 간단한 함수를 사용하도록 유도했다. (오른쪽)체중 감소가 0에 가까워지면(즉, 최소 정규화로 결정되지 않은 문제를 해결하기 위해 무어-펜로즈 의사 역수를 사용함) 그림 5.2에서 보았던 것처럼 도-9 다항식이 크게 적합해진다.

보다 일반적으로, 우리는 비용 함수에 정규화기라는 페널티를 추가하여 함수 f(x; (alpha))를 학습하는 모델을 정규화할 수 있다. 중량 붕괴의 경우, 정규화기는 (ohm)(w) = w^(T)w이다. 7장에서, 우리는 다른 많은 정규화기가 가능하다는 것을 알게 될 것이다.

한 함수에 대한 선호도를 다른 함수에 비해 표현하는 것이 가설 공간에서 멤버를 포함하거나 제외하는 것보다 모형의 용량을 제어하는 더 일반적인 방법입니다. 우리는 가설 공간에서 함수를 제외하는 것을 그 함수에 대해 무한히 강한 선호도를 표현하는 것으로 생각할 수 있다.

우리의 체중 감소 예제에서, 우리는 최소화 기준의 추가 항을 통해 더 작은 가중치로 정의된 선형 함수에 대한 선호도를 명시적으로 표현했다. 암묵적으로나 명시적으로 다른 솔루션에 대한 선호를 표현하는 많은 다른 방법이 있다. 이러한 다양한 접근 방식을 함께 정규화라고 한다. 정규화는 일반화 오류를 줄이려고 의도된 학습 알고리듬에 대한 수정이지만 훈련 오류는 수정하지 않는다. 정규화는 기계 학습 분야의 중심 관심사 중 하나이며 최적화에 의해서만 그 중요성에 맞섰다.

무료 점심식사 정리는 최고의 기계 학습 알고리듬이 없으며, 특히 정규화의 최상의 형태가 없다는 것을 분명히 했다. 대신 우리는 우리가 해결하고자 하는 특정 과제에 잘 맞는 정규화의 형태를 선택해야 한다. 일반적으로 딥 러닝의 철학과 특히 이 책은 매우 광범위한 작업(사람들이 할 수 있는 모든 지적 작업 등)이 모두 매우 범용적인 형태의 정규화를 사용하여 효과적으로 해결될 수 있다는 것이다.

