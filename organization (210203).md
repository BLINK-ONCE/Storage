# 5.4 Estimators, Bias and Variance

## 번역

통계 분야는 우리에게 훈련 세트뿐만 아니라 일반화 작업이라는 기계 학습 목표를 달성하는 데 사용할 수 있는 많은 도구를 제공한다. 모수 추정, 편향 및 분산과 같은 기본 개념은 일반화, 과소 적합 및 과적합 개념을 공식적으로 특성화하는 데 유용하다.

### Point Estimation

모수의 추정치와 참 값을 구별하기 위해, 우리의 관습은 모수의 점 추정치인 θˆ를 θ 로 표시하는 것이다.

{x^(1)}, ..., x^(m)}을(를) 독립적이고 동일한 분포의 m 집합으로 설정합니다.


알고리즘 5.1 k-폴드 교차 검증 알고리즘. 주어진 데이터 세트 D가 단순 열차/테스트 또는 열차/유효 분할에 비해 너무 작아 작은 테스트 세트에서 손실 L의 평균이 너무 높은 분산을 가질 수 있기 때문에 일반화 오류를 정확하게 추정할 수 없을 때 학습 알고리듬 A의 일반화 오류를 추정하는 데 사용할 수 있다. 데이터 세트 D는 (입력, 대상) 쌍 z^(i) = (x^(i), (y^(i) 또는 비지도 학습의 경우 입력 z(i) = x(i)를 나타낼 수 있는 추상적인 예를 요소로 포함한다. 알고리즘은 D의 각 예에 대한 오류 벡터를 반환합니다. 평균은 추정 일반화 오류입니다. 개별 예제의 오차는 평균 주위의 신뢰 구간(식 5.47)을 계산하는 데 사용할 수 있습니다. 교차 검증을 사용한 후에는 이러한 신뢰 구간이 잘 정당화되지 않지만, 알고리듬 A의 오류에 대한 신뢰 구간이 아래에 있고 알고리듬 B의 신뢰 구간과 교차하지 않는 경우에만 알고리듬 A가 알고리듬 B보다 낫다는 것을 선언하기 위해 그것들을 사용하는 것은 여전히 일반적인 관행이다.

(i.i.d.) 데이터 지점. 점 추정기 또는 통계량은 데이터의 함수입니다.

 θˆm = g(x^(1) , . . . , x^(m) )   (5.19)


정의상 참 θ에 가까운 값 또는 g의 범위가 허용 가능한 값 집합인 θ와 동일할 필요는 없다. 점 추정기의 이 정의는 매우 일반적이며 추정기의 설계자가 큰 유연성을 가질 수 있게 한다. 따라서 거의 모든 함수가 추정기로 적합하지만, 좋은 추정기는 출력이 훈련 데이터를 생성한 실제 기본 θ에 가까운 함수이다.


일단은 통계학에 대한 빈도주의적 관점을 취한다. 즉, 우리는 참 매개변수 값 θ가 고정되었지만 알 수 없다고 가정하고, 점 추정 θˆ는 데이터의 함수이다. 데이터는 랜덤 공정에서 추출되므로 데이터의 모든 함수는 랜덤입니다. 따라서 θˆ는 변량 변수입니다.


점 추정은 입력 변수와 대상 변수 간의 관계에 대한 추정도 참조할 수 있습니다. 우리는 이러한 유형의 점 추정치를 함수 추정기로 참조한다.


함수 추정 위에서 언급했듯이, 때때로 우리는 함수 추정(또는 함수 근사)을 수행하는 데 관심이 있다. 여기서 우리는 입력 벡터 x가 주어진 변수 y를 예측하려고 한다. 우리는 y와 x 사이의 근사 관계를 설명하는 함수 f(x)가 있다고 가정한다. 예를 들어, 우리는 y = f(x) + α라고 가정할 수 있습니다. 여기서 α는 x에서 예측할 수 없는 y 부분을 나타냅니다. 함수 추정에서, 우리는 모델 또는 추정치 βf를 사용하여 f를 근사화하는 데 관심이 있다. 함수 추정은 모수 α를 추정하는 것과 실제로 동일하다. 함수 추정기 βf는 함수 공간의 점 추정기일 뿐이다. 선형 회귀 예제(섹션 5.1.4에서 설명)와 다항식 회귀 예제(섹션 5.2에서 설명)는 모두 매개 변수 w를 추정하거나 x에서 y로 매핑하는 함수 β를 추정하는 것으로 해석될 수 있는 시나리오의 예제이다.

이제 우리는 점 추정기의 가장 일반적으로 연구된 속성을 검토하고 이러한 추정기에 대해 그들이 우리에게 무엇을 말하는지에 대해 논의한다.

### Bias

추정기의 편향은 다음과 같이 정의된다.

bias((θm)^) = E((θm)^) − θ


여기서 기대값은 데이터(임의변수에서 추출한 표본으로 나타남)이며, α는 데이터 생성 분포를 정의하는 데 사용되는 α의 실제 기본 값입니다. 추정기 αm은 치우침(αm) = 0이면 치우침이 없다고 하며, 이는 E(αm) = αm을 의미하며, 추정기 αm은 림m →α 바이어스(αm) = 0이면 점근적으로 치우침이 없다고 한다.

예: 베르누이 분포 평균 α를 가진 베르누이 분포에 따라 독립적이고 동일한 방식으로 분포하는 표본 {x(1)}, ..., x(m)} 집합을 고려합니다.   (5.21)

이 분포의 α 매개변수에 대한 일반적인 추정기는 훈련 샘플의 평균이다. (5.22)


이 추정기가 편향되었는지 여부를 결정하기 위해 방정식 5.22를 방정식 5.20으로 대체할 수 있다. (5.23, 24, 25, 26, 27, 28)


가우스 확률 밀도 함수는 다음과 같이 제공된다는 점을 상기합니다.


### 5.4.3 Variance and Standard Error


우리가 고려할 수 있는 추정기의 또 다른 특성은 데이터 샘플의 함수로서 얼마나 달라질 것으로 예상하느냐이다. 우리가 그것의 편향을 결정하기 위해 추정기의 기대치를 계산한 것처럼, 우리는 그것의 분산을 계산할 수 있다.

추정기의 분산은 단순히 분산입니다.


Var(θ^) (5.45)

여기서 랜덤 변수는 훈련 세트입니다. 또는 분산의 제곱근을 표준 오차(SE(ˆθ)라고 합니다.


추정기의 분산 또는 표준 오차는 기본 데이터 생성 프로세스로부터 데이터 세트를 독립적으로 샘플링할 때 데이터에서 계산한 추정치가 어떻게 달라질 것으로 예상할 수 있는가에 대한 척도를 제공한다. 추정기가 낮은 편향을 나타내기를 원할 수 있듯이, 우리는 또한 상대적으로 낮은 편향을 가지기를 원한다.

유한한 수의 표본을 사용하여 통계량을 계산할 때, 동일한 분포에서 다른 표본을 얻을 수 있었고 통계량이 달랐을 것이라는 점에서 진정한 기본 모수에 대한 추정치는 불확실하다. 추정기에서 예상되는 변동 정도는 우리가 정량화하고자 하는 오차의 원천이다.

The standard error of the mean is given by (5.46)

여기서 σ 2는 표본 x i의 실제적인 변화이다. 표준 오차는 흔히 σ의 추정치를 사용하여 추정한다. 불행히도, 표본 분산의 제곱근이나 분산의 치우치지 않은 추정기의 제곱근은 표준 편차에 대한 치우치지 않은 추정치를 제공하지 않습니다. 두 가지 접근법 모두 실제 표준 편차를 과소평가하는 경향이 있지만, 실제로 여전히 사용되고 있습니다. 편견이 없는 분산 추정기의 제곱근은 과소 추정치보다 작습니다. 큰 m의 경우 근사치가 상당히 합리적입니다.


평균의 표준 오차는 기계 학습 실험에서 매우 유용합니다. 우리는 종종 테스트 세트의 오차의 표본 평균을 계산하여 일반화 오류를 추정한다. 검정 집합의 예제 수에 따라 이 추정치의 정확도가 결정됩니다. 평균이 정규 분포와 함께 근사적으로 분포된다는 것을 알려주는 중심 한계 정리를 이용하여, 우리는 표준 오차를 사용하여 선택된 어떤 구간에서든 실제 기대치가 떨어질 확률을 계산할 수 있다. 예를 들어, 평균 µˆm을 중심으로 한 95% 신뢰 구간은 다음과 같습니다. (5.47)


평균 µm 및 분산 SE(μ^m) 2. 기계 학습 실험에서 알고리즘 A의 오류에 대한 95% 신뢰 구간의 상한값이 알고리즘 B의 오류에 대한 95% 신뢰 구간의 하한보다 작으면 알고리즘 A가 알고리즘 B보다 낫다고 말하는 것이 일반적이다.


Example: Bernoulli Distribution
(5.48, 49, 50, 51, 52)


추정기의 분산은 데이터 세트의 예제 수인 m의 함수로 감소한다. 이것은 일관성에 대해 논의할 때 되돌아갈 인기 있는 추정기의 공통 특성이다(5.4.5절 참조).


### 5.4.4 Trading off Bias and Variance to Minimize Mean Squared Error


치우침과 분산은 추정기에서 두 가지 서로 다른 오차 소스를 측정합니다. 치우침은 함수 또는 모수의 참 값에서 예상되는 편차를 측정합니다. 반면 분산은 데이터의 특정 표본 추출이 유발할 수 있는 기대 추정기 값으로부터의 편차에 대한 측도를 제공합니다.


치우침이 더 많은 추정치와 분산이 더 많은 두 추정치 사이에서 선택권이 주어지면 어떻게 됩니까? 둘 중 하나를 어떻게 선택할까요? 예를 들어, 그림 5.2에 표시된 함수를 근사화하는 데 관심이 있으며 편향이 큰 모델과 분산이 큰 모델 사이에서만 선택권이 제공된다고 가정해 보자. 둘 중 하나를 어떻게 선택할까요?


이 절충을 협상하는 가장 일반적인 방법은 교차 검증을 사용하는 것입니다. 경험적으로 교차 검증은 많은 실제 작업에서 매우 성공적이다. 또는 추정치의 평균 제곱 오차(MSE)도 비교할 수 있습니다. (5.53, 54)


MSE는 추정기와 모수 α의 참 값 사이의 전체 예상 편차(오차 제곱 의미)를 측정합니다. 방정식 5.54에서 분명히 알 수 있듯이, MSE를 평가하면 치우침과 분산이 모두 통합됩니다. 바람직한 추정치는 MSE가 작은 추정치이며 이러한 추정치는 치우침과 분산 모두를 어느 정도 억제하는 데 도움이 된다.


그림 5.6: 용량이 증가하면(x 축), 치우침(점선)이 감소하고 분산(대량선)이 증가하여 일반화 오류(대형 곡선)에 대한 또 다른 U자형 곡선이 생성된다. 한 축에 따라 용량을 달리하면 최적 용량이 존재하며, 용량이 이 최적 용량보다 낮을 때는 과소적합, 위에 있을 때는 과적합됩니다. 이 관계는 5.2절과 그림 5.3에서 설명한 용량, 과소적합 및 과적합 간의 관계와 유사합니다.


편향과 분산 사이의 관계는 용량, 과소적합 및 과적합이라는 기계 학습 개념과 밀접하게 연관되어 있다. 일반화 오차가 MSE에 의해 측정되는 경우(편향과 분산이 일반화 오차의 의미 있는 구성요소인 경우), 용량을 증가시키면 분산이 증가하고 편향이 감소하는 경향이 있다. 이는 그림 5.6에 나타나 있으며, 여기서 우리는 용량의 함수로 일반화 오류의 U자형 곡선을 다시 본다.


### Consistency


지금까지 우리는 고정된 크기의 훈련 세트에 대한 다양한 추정기의 속성에 대해 논의하였다. 일반적으로, 우리는 훈련 데이터의 양이 증가함에 따라 추정기의 동작에도 관심이 있다. 특히, 우리는 일반적으로 데이터 세트의 데이터 포인트 수가 증가함에 따라 포인트 추정치가 해당 매개 변수의 참 값으로 수렴되기를 바란다. 좀 더 공식적으로, 우리는 그것을 원합니다 (5.55)


### 5.5 Xaximum Likelihood Estimation

이전에, 우리는 공통 추정기의 몇 가지 정의를 보고 그 속성을 분석하였다. 그런데 이 추정기들은 어디서 왔을까요? 일부 함수가 좋은 추정기를 만들 수 있다고 추측하고 그 편향과 분산을 분석하는 대신, 우리는 다른 모델에 대한 좋은 추정기인 특정 함수를 도출할 수 있는 몇 가지 원칙을 가지고 싶다.


(중략)

(5.61) 이후 


이 KL 차이를 최소화하는 것은 분포 간의 교차 엔트로피를 최소화하는 것과 정확히 일치한다. 많은 저자들이 베르누이 또는 소프트맥스 분포의 음의 로그 우도를 식별하기 위해 "크로스 엔트로피"라는 용어를 사용하지만, 그것은 잘못된 명칭이다. 음의 로그 우도로 구성된 모든 손실은 훈련 세트에 의해 정의된 경험적 분포와 모델에 의해 정의된 확률 분포 사이의 교차 엔트로피이다. 예를 들어, 평균 제곱 오차는 경험적 분포와 가우스 모형 사이의 교차 엔트로피입니다.


따라서 모델 분포를 경험적 분포(^p) 데이터와 일치시키기 위한 시도로서 최대 가능성을 볼 수 있다. 이상적으로는 실제 데이터 생성 분포(p) 데이터를 일치시키고 싶지만 이 분포에 직접 액세스할 수 없습니다.


최적 α는 가능성을 최대화하거나 KL 분산을 최소화하는 것과 관계없이 동일하지만, 목표 함수의 값은 다르다. 소프트웨어에서, 우리는 종종 비용 함수를 최소화한다고 표현한다. 따라서 최대 가능성은 음의 로그 우도(NLL)의 최소화 또는 이에 상응하는 교차 엔트로피의 최소화가 된다. 최소 KL 발산에서의 최대우도 관점은 KL 발산에서 알려진 최소값이 0이기 때문에 이 경우에 도움이 된다. x가 실제 값일 때 음의 로그 가능성은 실제로 음수가 될 수 있습니다.
